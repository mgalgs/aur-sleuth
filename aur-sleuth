#!/usr/bin/env -S uv --quiet run --script
# -*- mode: python -*-
# /// script
# requires-python = "==3.12"
# dependencies = [
#     "openai",
#     "rich",
# ]
# ///

from collections import defaultdict
from enum import Enum
from pathlib import Path
from typing import Dict, Iterable, List, Tuple, Optional, NamedTuple
from urllib.parse import urlparse
import abc
import argparse
import configparser
import logging
import os
import pprint
import shutil
import subprocess
import sys
import tempfile
import time
import xml.etree.ElementTree as ET

from openai import OpenAI, APIError
from rich import box
from rich.console import Console, Group
from rich.live import Live
from rich.panel import Panel
from rich.spinner import Spinner
from rich.text import Text


# --- Logging Setup ---
LOG_FILE = "/tmp/aur-sleuth-debug.log"
# Clear the log file at the start of a run
if os.path.exists(LOG_FILE):
    os.remove(LOG_FILE)

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    filename=LOG_FILE,
    filemode="a",
)
logger = logging.getLogger("aur-sleuth")
# --- End Logging Setup ---


class Report:
    def __init__(self, report_path: Path):
        self.report_path = report_path

    def __enter__(self):
        self.file = open(self.report_path, "w", encoding="utf-8")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.file:
            self.file.close()

    def write(self, text: str, end: str = "\n", stdout=False):
        """Writes text to the report file."""
        if not hasattr(self, "file"):
            raise RuntimeError("Report file is not open. Use __entry__ to open it.")
        self.file.write(text + end)
        self.file.flush()
        if stdout:
            print(text, end=end)


class SafeStatus(Enum):
    SAFE = 1
    UNSAFE = 2
    SKIPPED = 3

    def get_color(self):
        return {
            self.SAFE: "green",
            self.UNSAFE: "red",
            self.SKIPPED: "yellow",
        }[self]

    def get_icon(self):
        return {
            self.SAFE: "✔",
            self.UNSAFE: "✖",
            self.SKIPPED: "!",
        }[self]


class AuditResult(NamedTuple):
    file_path: Optional[Path]
    pkgdir: Path
    status: SafeStatus
    summary: str
    details: str = ""

    def __str__(self):
        deets = f" / {self.details}" if self.details else ""
        fpath = f"{self.file_path}: " if self.file_path else ""
        return f"{fpath}[{self.status.name}] {self.summary}{deets}"

    def report_text(self, use_color=True) -> str:
        header = (
            self.file_path.relative_to(self.pkgdir).name
            if self.file_path
            else self.summary
        )
        content = f"## {header}\n\n"

        if use_color:
            color = self.status.get_color()
            content += f"Status: [{color}] {self.status.name}[/{color}]\n\n"
        else:
            content += f"Status: {self.status.name}\n\n"

        content += f"Summary: {self.summary}\n\n"

        if self.details:
            content += f"Details:\n\n{self.details}\n\n"

        return content


def partition_results_by_status(
    results: List[AuditResult],
) -> Dict[SafeStatus, List[AuditResult]]:
    results_by_status: dict[SafeStatus, List[AuditResult]] = defaultdict(list)

    for result in results or []:
        results_by_status[result.status].append(result)

    return results_by_status


def generate_report_text(
    report_path: Path,
    results_by_status: Dict[SafeStatus, List[AuditResult]],
    use_color: bool = False,
    execution_time: Optional[float] = None,
) -> str:
    content = ""
    nunsafe = len(results_by_status[SafeStatus.UNSAFE])
    if nunsafe > 0:
        content += f"# Issues ({nunsafe} total)\n\n"
        for issue in results_by_status[SafeStatus.UNSAFE]:
            content += issue.report_text(use_color=use_color) + "---\n\n"

    nskips = len(results_by_status[SafeStatus.SKIPPED])
    if nskips > 0:
        skipped_files = ", ".join(
            (r.file_path.name if r.file_path else "?")
            for r in results_by_status[SafeStatus.SKIPPED]
        )
        content += (
            f"(Skipped {nskips} file{'' if nskips == 1 else 's'}: {skipped_files})\n\n"
        )

    # Add execution time to the report if available
    if execution_time is not None:
        content += f"Total execution time: {execution_time:.2f} seconds\n\n"

    content += f"Full audit report can be found in {report_path}"

    return content


# --- TUI Setup ---
class ConsoleUI(abc.ABC):
    """Base class for terminal user interfaces."""

    def __init__(self, report: Report):
        raise NotImplementedError("Subclasses must implement __init__.")

    def update_status(self, text):
        """Updates the status text."""
        raise NotImplementedError("Subclasses must implement update_status.")

    def finalize_step(self, message, success=True, status: Optional[SafeStatus] = None):
        """Finalizes a step with a message."""
        raise NotImplementedError("Subclasses must implement finalize_step.")

    def show_summary(
        self,
        report_path: Path,
        audit_results: Optional[List[AuditResult]] = None,
        execution_time: Optional[float] = None,
    ):
        """Displays the final audit summary."""
        raise NotImplementedError("Subclasses must implement show_summary.")

    @property
    def has_color(self):
        return False


class TUIPlain(ConsoleUI):
    def __init__(self, report: Report):
        self.report = report

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

    def update_status(self, text):
        self.report.write(text, stdout=True)

    def finalize_step(self, message, success=True, status: Optional[SafeStatus] = None):
        icon = status.get_icon()
        self.report.write(f"{icon} {message}", stdout=True)

    def show_summary(
        self,
        report_path: Path,
        audit_results: Optional[List[AuditResult]] = None,
        execution_time: Optional[float] = None,
    ):
        results_by_status = partition_results_by_status(audit_results or [])

        overall_status = (
            SafeStatus.UNSAFE
            if len(results_by_status[SafeStatus.UNSAFE]) > 0
            else SafeStatus.SAFE
        )

        content = generate_report_text(
            report_path,
            results_by_status,
            use_color=False,
            execution_time=execution_time,
        )

        recommended_action = (
            " -- DO NOT INSTALL!" if overall_status == SafeStatus.UNSAFE else ""
        )
        self.report.write(
            f"Audit complete! Result: {overall_status.name}{recommended_action}",
            stdout=True,
        )
        self.report.write(content, stdout=True)


class TUI(ConsoleUI):
    """Handles the terminal user interface."""

    def __init__(self, report: Report):
        self.console = Console()
        self.live = None
        self.spinner = Spinner("dots", text="")
        self.report = report
        self.history = []

    def __enter__(self):
        self.live = Live(
            self._get_renderable(),
            console=self.console,
            screen=False,
            auto_refresh=True,
        )
        self.live.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.live:
            self.live.stop()
            # Clear the line where the spinner was
            self.console.print("\r", end="")

    @property
    def has_color(self):
        return True

    def _get_renderable(self):
        """Constructs the renderable to be displayed in the Live object."""
        renderables = []
        if self.history:
            history_text = Text.from_markup("\n".join(self.history))
            history_panel = Panel(history_text, box=box.ROUNDED, title="Audit Log")
            renderables.append(history_panel)

        renderables.append(self.spinner)
        return Group(*renderables)

    def update_status(self, text):
        """Updates the status text within the live display."""
        if not self.live or not self.live.is_started:
            raise RuntimeError("TUI is not started. Call __enter__() first.")
        self.report.write(text)
        self.spinner.text = Text(f" {text}", style="bold blue")
        self.live.update(self._get_renderable())

    def finalize_step(self, message, success=True, status: Optional[SafeStatus] = None):
        """Adds a message to the history and updates the display."""
        status = status or SafeStatus.SAFE
        icon = status.get_icon()
        color = status.get_color()

        self.history.append(f"[{color}]{icon}[/{color}] {message}")
        self.report.write(message)
        # Update the live display with the new history
        self.live.update(self._get_renderable())

    def show_summary(
        self,
        report_path: Path,
        audit_results: Optional[List[AuditResult]] = None,
        execution_time: Optional[float] = None,
    ):
        """Displays the final audit summary in a box."""
        if self.live and self.live.is_started:
            # Trash the spinner
            self.spinner = Text("")
            self.live.update(self._get_renderable())
            self.live.stop()

        results_by_status = partition_results_by_status(audit_results or [])
        overall_status = (
            SafeStatus.UNSAFE
            if len(results_by_status[SafeStatus.UNSAFE]) > 0
            else SafeStatus.SAFE
        )
        result_color = overall_status.get_color()

        recommended_action = (
            " -- DO NOT INSTALL!" if overall_status == SafeStatus.UNSAFE else ""
        )
        title = f"Audit complete! Result: [{result_color}]{overall_status.name}{recommended_action}[/{result_color}]"

        content = generate_report_text(
            report_path,
            results_by_status,
            use_color=True,
            execution_time=execution_time,
        )

        self.report.write(content)

        panel = Panel(content, title=title, box=box.ROUNDED, expand=False)
        self.console.print(panel)


# --- End TUI Setup ---


# Default configuration
DEFAULT_MODEL = "qwen/qwen3-30b-a3b-instruct-2507"
SESSION_AUDIT_LIMIT_TOKENS = 100_000  # 100k tokens for the entire session
IGNORED_DIRS = [".git"]

SYSTEM_PROMPTS = {
    "general_security_auditor": lambda: (
        """You are an agentic security auditor. Your goal is to inspect the source code and AUR build files in this package to find any potential vulnerabilities, malicious code, or supply chain attack vectors."""
    ),
    "file_auditor": lambda package_name: (
        f"""You are a security expert tasked with auditing a file from a package distributed via the Arch User Repository (AUR).
This file is part of the {package_name} package for Arch Linux systems. Recently, there have been supply chain attacks where
malicious code was inserted into AUR packages in subtle ways. You need to detect any potential security issues in this file.

Please carefully analyze the following file and identify any potential security issues, including but not limited to:
1. Suspicious network requests or downloads from non-standard sources
2. Obfuscated code or unusual encoding
3. Unexpected file operations or system modifications
4. Use of potentially dangerous commands like eval, base64, curl, wget in unexpected contexts
5. Anything that deviates from standard packaging practices
"""
    ),
}


def remove_thinking_block(response: str) -> str:
    if response.startswith("<think>"):
        end_idx = response.find("</think>")
        if end_idx != -1:
            return response[end_idx + len("</think>") :]
    return response


class LLM:
    """LLM interface"""

    def __init__(
        self,
        limit_tokens,
        client,
        model=DEFAULT_MODEL,
        system_prompt=None,
        temperature=0.0,
        top_p=0.1,
    ):
        self.limit_tokens = limit_tokens
        self.tokens_processed = 0
        self.client = client
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.history = []
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        if system_prompt:
            self.init_chat(system_prompt)

    @property
    def limit_reached(self):
        return self.tokens_processed >= self.limit_tokens

    def add_tokens(self, num_tokens):
        """
        Adds tokens to the processed total and checks if the limit is reached.
        """
        self.tokens_processed += num_tokens

    def init_chat(self, system_prompt):
        """Initializes a chat session with a system prompt."""
        logger.debug("Initializing chat session with system prompt: %s", system_prompt)
        self.history = [{"role": "system", "content": system_prompt}]

    def push_user_message(self, content):
        """Adds a user message to the chat history."""
        logger.debug("Pushing user message to chat history: %s", content)
        self.history.append({"role": "user", "content": content})

    def chat(self, user_prompt):
        """Makes an LLM call and tracks token usage."""
        if not self.history:
            raise ValueError("Chat session not initialized. Call init_chat first.")

        self.push_user_message(user_prompt)

        kwargs = {
            "model": self.model,
            "messages": self.history,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "timeout": (5.0, 240.0),  # 5s connect, 240s read
        }
        response = self.client.chat.completions.create(**kwargs)
        if response.usage:
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens
            self.add_tokens(self.prompt_tokens + self.completion_tokens)
        rsp_msg = response.choices[0].message
        self.history.append(rsp_msg)
        logger.debug("--- BEGIN LLM REQUEST ---")
        logger.debug("%s", pprint.pformat(self.history[-2]))
        logger.debug("--- END LLM REQUEST ---")
        logger.debug("--- BEGIN LLM RESPONSE ---")
        logger.debug("%s", pprint.pformat(self.history[-1]))
        logger.debug("--- END LLM RESPONSE ---")
        content = rsp_msg.content
        return remove_thinking_block(content)


def get_api_key():
    """Get API key from OPENAI_API_KEY environment variable"""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("ERROR: OPENAI_API_KEY environment variable not set", file=sys.stderr)
        sys.exit(1)
    return api_key


def get_base_url():
    """Get API endpoint from OPENAI_BASE_URL, with fallback to OpenRouter"""
    base_url = os.environ.get("OPENAI_BASE_URL")
    if base_url:
        return base_url
    print("WARN: OPENAI_BASE_URL not set, using OpenRouter as fallback")
    return "https://openrouter.ai/api/v1"


def file_is_plain_text(file_path: Path) -> bool:
    """Detects if a file is text or binary by sampling its content."""
    try:
        with open(file_path, "rb") as f:
            # Read a chunk of data (first 1024 bytes is usually enough)
            chunk = f.read(1024)
            if not chunk:  # Empty file
                return True

            # If null byte is present, likely binary
            if b"\x00" in chunk:
                return False

            # Optionally: count printable chars vs. non-printable
            # Using heuristic: if >30% are non-printable, consider binary
            nontext_ratio = sum(
                1 for b in chunk if b < 32 and b not in (9, 10, 13)
            ) / len(chunk)
            if nontext_ratio > 0.3:
                return False

            return True
    except Exception as e:
        logger.error(f"Failed to detect file type for {file_path}: {e}")
        return False


def audit_file(
    report: Report,
    package_name: str,
    file_path: Path,
    pkgdir: Path,
    client: OpenAI,
    tui: ConsoleUI,
) -> AuditResult:
    """Audits a single source file with an LLM."""

    # If the file is binary or too large, skip it
    is_plain_text = file_is_plain_text(file_path)
    if not is_plain_text:
        # TODO: Take a look at binary files?
        return AuditResult(
            file_path,
            pkgdir,
            SafeStatus.SKIPPED,
            f"Skipping binary file: {file_path.name}",
        )

    try:
        with open(file_path, "r") as f:
            file_content = f.read()
    except FileNotFoundError:
        return AuditResult(
            file_path, pkgdir, SafeStatus.UNSAFE, f"File not found at: {file_path}"
        )
    except Exception as e:
        return AuditResult(
            file_path, pkgdir, SafeStatus.UNSAFE, f"Failed to read file: {e}"
        )

    model = os.environ.get("OPENAI_MODEL", DEFAULT_MODEL)
    llm_pkgbuild_auditor = LLM(
        limit_tokens=SESSION_AUDIT_LIMIT_TOKENS,
        client=client,
        model=model,
        system_prompt=SYSTEM_PROMPTS["file_auditor"](package_name),
    )
    prompt = f"""Respond with a security assessment in the following XML format:
<security_assessment>
<decision>SAFE or UNSAFE</decision>
<details>
[Your detailed analysis here in markdown format. If UNSAFE, show a short snippet of the problematic code and explain exactly what is problematic and why. Keep it to 4 paragraphs or fewer.]
</details>
<summary>
[Brief summary of your analysis (12 words or less)]
</summary>
</security_assessment>

<file name="{file_path.name}">
{file_content}
</file>"""
    tui.update_status(f"Auditing {file_path.name} with {model}...")

    logger.debug("--- LLM REQUEST ---")
    logger.debug(prompt)

    try:
        assessment = llm_pkgbuild_auditor.chat(prompt)

        logger.debug("--- LLM RESPONSE ---")
        logger.debug(assessment)
        report.write(f"LLM auditresponse for {file_path.name}:\n{assessment}\n")

        root = ET.fromstring(assessment)

        decision_el = root.find("decision")
        if decision_el is None or decision_el.text is None:
            raise RuntimeError(
                "Malformed LLM response: <decision> element not found or empty."
            )
        decision = decision_el.text.strip().upper()

        details_el = root.find("details")
        if details_el is None or details_el.text is None:
            raise RuntimeError(
                "Malformed LLM response: <details> element not found or empty."
            )
        details = details_el.text.strip()

        summary_el = root.find("summary")
        if summary_el is None or summary_el.text is None:
            raise RuntimeError(
                "Malformed LLM response: <summary> element not found or empty."
            )
        summary = summary_el.text.strip()

        safestatus = SafeStatus.SAFE if decision == "SAFE" else SafeStatus.UNSAFE
        return AuditResult(file_path, pkgdir, safestatus, summary, details)

    except (APIError, ET.ParseError, AttributeError) as e:
        tui.finalize_step(f"LLM response processing failed: {e}", success=False)
        return AuditResult(file_path, pkgdir, SafeStatus.UNSAFE, str(e))


def gen_user_prompt_for_agentic_audit(
    required_review_files, other_pkg_files, already_reviewed_files
):
    required_review_files_str = "\n".join(f"- {f}" for f in required_review_files)
    other_pkg_files_str = "\n".join(
        f"- {f}" for f in other_pkg_files if f not in required_review_files
    )
    already_reviewed_files_str = "\n".join(f"- {f}" for f in already_reviewed_files)

    return f"""
The following files MUST be reviewed before making a decision:
<required_review_files>
{required_review_files_str}
</required_review_files>

The following files have already been reviewed:
<already_reviewed_files>
{already_reviewed_files_str}
</already_reviewed_files>

The following files are available for review:
<other_pkg_files>
{other_pkg_files_str}
</other_pkg_files>

You may now continue the review by selecting a file to read from the <required_review_files> or <other_pkg_files> sections. Please ensure you have read all files in <required_review_files> before making a final decision. You should also review all relevant files in <other_pkg_files> that you deem necessary or relevant.

You can now use the 'readfile' tool to read the content of any file you want to inspect, or output your final decision if you're done auditing files.
"""


def is_remote_url(source):
    # Common remote URL schemes
    remote_schemes = {
        "http",
        "https",
        "ftp",
        "ftps",
        "git",
        "ssh",
        "sftp",
        "rsync",
        "scp",
    }

    parsed = urlparse(source)
    return parsed.scheme in remote_schemes


def download_sources(tui: ConsoleUI):
    tui.update_status("Running makepkg --nobuild to download sources for agent...")
    try:
        subprocess.run(
            ["makepkg", "--nobuild"],
            check=True,
            capture_output=True,
            text=True,
        )
        tui.finalize_step("makepkg --nobuild successful")
        return True
    except subprocess.CalledProcessError as e:
        tui.finalize_step(
            "makepkg --nobuild failed.",
            success=False,
        )
        logger.error(f"makepkg --nobuild failed: {e.stderr}")
        return False


def get_source_info():
    output = subprocess.run(
        ["makepkg", "--printsrcinfo"],
        check=True,
        capture_output=True,
        text=True,
    )
    source_info = dict()
    for line in output.stdout.splitlines():
        line = line.strip()
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip()
        source_info[key] = value

    return source_info


def get_source_listing(
    pkgdir: Path, tui: ConsoleUI
) -> Tuple[Optional[List[Path]], Optional[List[Path]]]:
    required_review_files = [Path("PKGBUILD")]
    other_pkg_files = []
    seen_files = {Path("PKGBUILD").resolve()}

    # Use `makepkg --printsrcinfo` to get the PKGBUILD source files. These are always
    # required to be reviewed.
    tui.update_status("Running makepkg --printsrcinfo to get source files for agent...")
    try:
        source_info = get_source_info()
        for key, value in source_info.items():
            if key == "source":
                pfile = Path(value)
                if is_remote_url(value):
                    # Just use the basename since it should be downloaded to pkgdir
                    pfile = Path(pfile.name)
                if pfile.resolve() not in seen_files:
                    required_review_files.append(pfile)
                    seen_files.add(pfile.resolve())
        tui.finalize_step("makepkg --printsrcinfo successful")
    except subprocess.CalledProcessError as e:
        tui.finalize_step(
            "makepkg --printsrcinfo failed.",
            success=False,
        )
        logger.error(f"makepkg --printsrcinfo failed: {e.stderr}")
        return None, None

    # Create a recursive directory listing to pass to the audit agent
    for root, dirs, files in os.walk(pkgdir):
        rel_root = Path(os.path.relpath(root, pkgdir))
        for ignoramus in IGNORED_DIRS:
            if ignoramus in dirs:
                dirs.remove(ignoramus)
        for f in files:
            pkg_file = rel_root / f
            if pkg_file.resolve() in seen_files:
                continue
            other_pkg_files.append(pkg_file)
            seen_files.add(pkg_file.resolve())

    return required_review_files, other_pkg_files


def decide_next_files_to_review(
    report: Report,
    package_name: str,
    client: OpenAI,
    other_pkg_files: List[Path],
    already_reviewed_files: List[Path],
    num_additional_files_to_review: int,
) -> List[Path]:
    files_to_consider = [f for f in other_pkg_files if f not in already_reviewed_files]
    if len(files_to_consider) <= num_additional_files_to_review:
        return files_to_consider

    llm = LLM(
        limit_tokens=SESSION_AUDIT_LIMIT_TOKENS,
        client=client,
        model=os.environ.get("OPENAI_MODEL", DEFAULT_MODEL),
        system_prompt=SYSTEM_PROMPTS["general_security_auditor"](),
    )
    prompt = f"""
To continue the audit of the {package_name} package, please select {num_additional_files_to_review} more source files to review from the package directory listing below.

Do not select any files that have already been reviewed.

<already_reviewed_files>
{"- ".join(str(f) for f in already_reviewed_files)}
</already_reviewed_files>

<listing>
{"- ".join(str(f) for f in files_to_consider)}
</listing>

Respond with a list of file paths, one path per line, with NO other additional text or formatting.
"""
    response = llm.chat(prompt)
    if not response:
        raise RuntimeError("Failed to get response from LLM for file selection.")

    report.write(f"LLM response for file selection:\n{response}")

    files_to_review = []
    for line in response.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            file_path = Path(line)
            if file_path not in other_pkg_files:
                logger.warning("Invalid file path in LLM response: %s", line)
                continue
            files_to_review.append(file_path)
        except ValueError:
            logger.warning("Invalid file path in LLM response: %s", line)

    return files_to_review


def do_agentic_audit(
    tui: ConsoleUI, report: Report, package_name: str, client: OpenAI, pkgdir: Path
) -> List[AuditResult]:
    """Performs an agentic security audit on the package contents."""

    if not download_sources(tui):
        msg = "makepkg --nobuild failed, unable to download sources."
        report.write(msg)
        return [AuditResult(None, pkgdir, SafeStatus.SKIPPED, msg)]

    required_review_files, other_pkg_files = get_source_listing(pkgdir, tui)

    if required_review_files is None or other_pkg_files is None:
        msg = "Failed to get source files for agentic audit."
        report.write(msg)
        return [AuditResult(None, pkgdir, SafeStatus.SKIPPED, msg)]

    already_reviewed_files = []
    num_additional_files_to_review = 10
    audit_results: List[AuditResult] = []

    tui.update_status("Starting agentic audit...")

    tui.update_status("Reviewing required files...")
    for f in required_review_files:
        f_abs = f.resolve()
        f_rel = f_abs.relative_to(pkgdir)
        tui.update_status(f"Reviewing {f_rel}...")
        result = audit_file(report, package_name, f_abs, pkgdir, client, tui)
        status_txt = "Status: "
        if tui.has_color:
            color = result.status.get_color()
            status_txt += f"[{color}]{result.status.name}[/{color}] -- {result.summary}"
        else:
            status_txt += f"{result.status.name} -- {result.summary}"

        tui.finalize_step(f"Reviewed {f_rel}. {status_txt}", status=result.status)
        already_reviewed_files.append(f)
        audit_results.append(result)

    tui.finalize_step("Reviewed all required files.")

    tui.update_status("Deciding which files to review next...")
    files_to_review = decide_next_files_to_review(
        report,
        package_name,
        client,
        other_pkg_files,
        already_reviewed_files,
        num_additional_files_to_review,
    )
    tui.finalize_step(
        f"Decided to review {len(files_to_review)} additional files: {', '.join(str(f) for f in files_to_review)}"
    )

    tui.update_status("Reviewing additional files...")
    for f in files_to_review:
        f_abs = f.resolve()
        f_rel = f_abs.relative_to(pkgdir)
        tui.update_status(f"Reviewing {f_rel}...")
        result = audit_file(report, package_name, f_abs, pkgdir, client, tui)
        status_txt = "Status: "
        if tui.has_color:
            color = result.status.get_color()
            status_txt += f"[{color}]{result.status.name}[/{color}] -- {result.summary}"
        else:
            status_txt += f"{result.status.name} -- {result.summary}"

        tui.finalize_step(f"Reviewed {f_rel}. {status_txt}", status=result.status)
        audit_results.append(result)

    tui.finalize_step("Reviewed additional files.")

    return audit_results


def run_aur_sleuth_audit(
    tui: ConsoleUI,
    report: Report,
    package_name: str,
    client: OpenAI,
    pkgdir: Path,
) -> int:
    """Runs the specified security audits for aur-sleuth."""
    report_file = report.report_path
    audit_results = []
    audit_ok = True
    model = os.environ.get("OPENAI_MODEL", DEFAULT_MODEL)

    # Track execution time
    start_time = time.time()
    execution_time = None

    try:
        tui.finalize_step(
            f"Analyzing {package_name} AUR package with {model} from {get_base_url()}"
        )

        os.chdir(pkgdir)

        audit_results = do_agentic_audit(tui, report, package_name, client, pkgdir)

    except subprocess.CalledProcessError as e:
        msg = f"An unexpected error occurred: {e}"
        tui.finalize_step(msg, success=False)
        audit_results.append(AuditResult(None, pkgdir, SafeStatus.SKIPPED, msg))
    except Exception as e:
        msg = f"An unexpected error occurred: {e}"
        logger.error(msg, exc_info=True)
        tui.finalize_step(msg, success=False)
        audit_results.append(AuditResult(None, pkgdir, SafeStatus.SKIPPED, msg))
        raise
    finally:
        execution_time = time.time() - start_time
        tui.show_summary(report_file, audit_results, execution_time)

        # Write full report
        results_by_status = partition_results_by_status(audit_results)
        overall_status = (
            SafeStatus.UNSAFE
            if len(results_by_status[SafeStatus.UNSAFE]) > 0
            else SafeStatus.SAFE
        )
        report.write(f"Final Status: {overall_status.name}\n\n")
        if results_by_status[SafeStatus.UNSAFE]:
            audit_ok = False
            report.write("Issues Found:\n")
            for issue in results_by_status[SafeStatus.UNSAFE]:
                report.write(str(issue) + "\n")
        else:
            report.write("No issues found.\n")

        if results_by_status[SafeStatus.SKIPPED]:
            report.write("\nAudit Skips:" + "\n")
            for skip in results_by_status[SafeStatus.SKIPPED]:
                report.write(str(skip) + "\n")

    return 0 if audit_ok else 1


def download_package_to_tmpdir(
    tui: ConsoleUI,
    tmpdir_parent: Path,
    package_name: str,
    clone_url: Optional[str] = None,
) -> Path:

    # Create a temp directory inside it
    tmpdir = tempfile.mkdtemp(prefix="aur-sleuth-", dir=tmpdir_parent)

    tui.update_status(
        f"Cloning https://aur.archlinux.org/{package_name}.git to {tmpdir}..."
    )
    clone_url = clone_url or f"https://aur.archlinux.org/{package_name}.git"
    subprocess.run(
        ["git", "clone", clone_url, tmpdir],
        check=True,
        capture_output=True,
        text=True,
        timeout=30,
    )
    tui.finalize_step(f"Cloned repository to {tmpdir}")
    return Path(tmpdir).resolve()


def sleuth_main() -> int:
    parser = argparse.ArgumentParser(
        description="Run a security audit on an AUR package."
    )
    parser.add_argument("package_name", help="Name of the AUR package.")
    parser.add_argument(
        "--clone-url",
        default=None,
        help="Optional custom clone URL for the AUR package. Defaults to https://aur.archlinux.org/{package_name}.git.",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Output format. Supported formats: rich, plain. Defaults to rich.",
    )
    parser.add_argument(
        "--model",
        default=None,
        help="LLM to use (overrides environment and config file settings)",
    )
    parser.add_argument(
        "--base-url",
        default=None,
        help="Base API URL (OpenAI API compatible) to use (overrides environment and config file settings)",
    )
    args = parser.parse_args()

    if args.model:
        logger.debug("Setting model from command line: %s", args.model)
        os.environ["OPENAI_MODEL"] = args.model

    if args.base_url:
        logger.debug("Setting base URL from command line: %s", args.base_url)
        os.environ["OPENAI_BASE_URL"] = args.base_url

    client = get_openai_client()

    original_cwd = os.getcwd()

    TuiCls = TUIPlain if args.output == "plain" else TUI

    # Create the base parent directory
    tmpdir_parent = Path(tempfile.gettempdir()) / "aur-sleuth"
    os.makedirs(tmpdir_parent, exist_ok=True)

    report_file = tmpdir_parent / f"aur-sleuth-report-{args.package_name}.txt"

    with Report(report_file) as report:
        with TuiCls(report) as tui:
            pkgdir = download_package_to_tmpdir(
                tui, Path(tmpdir_parent), args.package_name, args.clone_url
            )
            retval = run_aur_sleuth_audit(
                tui, report, args.package_name, client, pkgdir
            )

    os.chdir(original_cwd)
    if os.path.exists(pkgdir):
        shutil.rmtree(pkgdir)

    return retval


def wrapper_main() -> int:
    parser = argparse.ArgumentParser(
        description="Wrapper for makepkg to audit PKGBUILD."
    )
    args, makepkg_args = parser.parse_known_args()

    # Arguments that indicate a non-build action where we should skip the audit
    skip_audit_flags = ["--verifysource", "--nobuild", "--geninteg", "-o", "-g"]

    # Check if any of the skip flags are in the arguments passed to the wrapper
    if any(flag in makepkg_args for flag in skip_audit_flags):
        # If so, just execute makepkg without an audit
        os.execv("/usr/bin/makepkg", ["/usr/bin/makepkg"] + makepkg_args)
        return 0

    client = get_openai_client()
    source_info = get_source_info()
    package_name = source_info["pkgname"]

    # Create a temporary report file for the wrapper
    tmpdir_parent = Path(tempfile.gettempdir()) / "aur-sleuth"
    os.makedirs(tmpdir_parent, exist_ok=True)
    report_file = tmpdir_parent / f"aur-sleuth-report-{package_name}-wrapper.txt"

    with Report(report_file) as report:
        with TUI(report) as tui:
            retval = run_aur_sleuth_audit(
                tui, report, package_name, client, Path(os.getcwd())
            )

    if retval != 0:
        print("Security audit failed. Exiting makepkg wrapper.")
        return retval

    print("Security audit passed. Proceeding with makepkg execution...")
    os.execv("/usr/bin/makepkg", ["/usr/bin/makepkg"] + makepkg_args)
    return 0


def get_openai_client() -> OpenAI:
    base_url = get_base_url()
    default_headers = {}
    if "openrouter.ai" in base_url:
        default_headers = {
            "HTTP-Referer": "https://github.com/mgalgs/aur-sleuth",
            "X-Title": "aur-sleuth",
        }

    return OpenAI(
        api_key=get_api_key(), base_url=base_url, default_headers=default_headers
    )


def load_config():
    """Load configuration from system and user config files."""
    config_files = [
        Path("/etc/aur-sleuth.conf"),
        Path.home() / ".config" / "aur-sleuth.conf",
    ]

    # Read all existing config files (later files override earlier ones)
    for config_file in config_files:
        config = configparser.ConfigParser()
        if config_file.exists():
            try:
                config.read(config_file)
                logger.debug("Loading settings from %s", config_file)
            except Exception as e:
                logger.warning(f"Failed to read config from {config_file}: {e}")

        if config.has_section("default"):
            config_section = config["default"]
            for key in ["OPENAI_API_KEY", "OPENAI_BASE_URL", "OPENAI_MODEL"]:
                if key in config_section:
                    logger.debug("Setting %s from %s", key, config_file)
                    os.environ[key] = config_section[key]


def main():
    # Load configuration files if they exist
    load_config()

    invocation_name = Path(sys.argv[0]).name

    if invocation_name == "aur-sleuth":
        sys.exit(sleuth_main())
    elif invocation_name == "aur-sleuth-makepkg-wrapper":
        sys.exit(wrapper_main())
    else:
        print(
            f"ERROR: Unknown invocation name '{invocation_name}'. Please invoke as 'aur-sleuth' or 'aur-sleuth-makepkg-wrapper'.",
            file=sys.stderr,
        )
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nAudit interrupted by user.", file=sys.stderr)
        sys.exit(1)
