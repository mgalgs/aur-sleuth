#!/usr/bin/env -S uv --quiet run --script
# -*- mode: python -*-
# /// script
# requires-python = "==3.12"
# dependencies = [
#     "openai",
# ]
# ///

import os
import sys
import argparse
import tempfile
import shutil
import subprocess
from pathlib import Path
import logging
import xml.etree.ElementTree as ET
from openai import OpenAI, APIError

# Default configuration
DEFAULT_MODEL = "qwen/qwen3-30b-a3b-instruct-2507"
MODEL = os.environ.get("OPENAI_MODEL", os.environ.get("MODEL", DEFAULT_MODEL))
SESSION_AUDIT_LIMIT_BYTES = 100 * 1024  # 100KB
LOG_FILE = "/tmp/aur-sleuth-debug.log"

# --- Logging Setup ---
# Clear the log file at the start of a run
if os.path.exists(LOG_FILE):
    os.remove(LOG_FILE)

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    filename=LOG_FILE,
    filemode="a",
)
logger = logging.getLogger("aur-sleuth")
# --- End Logging Setup ---


class AuditSession:
    """Tracks the state of an audit session."""

    def __init__(self, limit_bytes, client):
        self.limit_bytes = limit_bytes
        self.bytes_processed = 0
        self.limit_reached = False
        self.client = client
        self.prompt_tokens = 0
        self.completion_tokens = 0

    def add_bytes(self, num_bytes):
        """Adds bytes to the processed total and checks if the limit is reached."""
        if self.limit_reached:
            return
        self.bytes_processed += num_bytes
        if self.bytes_processed >= self.limit_bytes:
            self.limit_reached = True
            print(
                f"WARN: Session audit limit of {self.limit_bytes} bytes reached.",
                file=sys.stderr,
            )

    def audit_prompt(self, **kwargs):
        """Makes an audit call to the LLM and tracks token usage."""
        response = self.client.chat.completions.create(**kwargs)
        if response.usage:
            self.prompt_tokens += response.usage.prompt_tokens
            self.completion_tokens += response.usage.completion_tokens
        return response

    def get_llm_assessment(self, prompt_content):
        """Gets a security assessment from the LLM."""
        response = self.audit_prompt(
            model=MODEL,
            messages=[{"role": "user", "content": prompt_content}],
            temperature=0.0,
            top_p=0.1,
        )
        return response.choices[0].message.content

    def print_token_usage(self):
        """Prints the total token usage for the session."""
        total_tokens = self.prompt_tokens + self.completion_tokens
        print("\n--- Token Usage ---")
        print(f"Prompt tokens: {self.prompt_tokens}")
        print(f"Completion tokens: {self.completion_tokens}")
        print(f"Total tokens: {total_tokens}")
        print("--------------------")


def get_api_key():
    """Get API key from OPENAI_API_KEY environment variable"""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("ERROR: OPENAI_API_KEY environment variable not set", file=sys.stderr)
        sys.exit(1)
    return api_key


def get_base_url():
    """Get API endpoint from OPENAI_BASE_URL, with fallback to OpenRouter"""
    base_url = os.environ.get("OPENAI_BASE_URL")
    if base_url:
        if not base_url.endswith("/v1"):
            base_url += "/v1"
        return base_url
    print("WARN: OPENAI_BASE_URL not set, using OpenRouter as fallback")
    return "https://openrouter.ai/api/v1"


def audit_pkgbuild(pkgbuild_path, session, makepkg_args=None, standalone=False):
    """Audits a PKGBUILD file with an LLM."""
    if makepkg_args is None:
        makepkg_args = ["/usr/bin/makepkg"] + sys.argv[1:]
    try:
        with open(pkgbuild_path, "r") as f:
            pkgbuild_content = f.read()
    except FileNotFoundError:
        print(f"ERROR: PKGBUILD not found at: {pkgbuild_path}", file=sys.stderr)
        return "UNSAFE"
    except Exception as e:
        print(f"ERROR: Failed to read PKGBUILD: {e}", file=sys.stderr)
        return "UNSAFE"

    prompt = f"""You are a security expert tasked with auditing a PKGBUILD file from the Arch User Repository (AUR).
This file is used to build packages on Arch Linux systems. Recently, there have been supply chain attacks where
malicious code was inserted into AUR packages in subtle ways.

Please carefully analyze the following PKGBUILD file and identify any potential security issues, including but not limited to:
1. Suspicious network requests or downloads from non-standard sources
2. Obfuscated code or unusual encoding
3. Unexpected file operations or system modifications
4. Use of potentially dangerous commands like eval, base64, curl, wget in unexpected contexts
5. Anything that deviates from standard packaging practices

Respond with a security assessment in the following XML format:
<security_assessment>
<decision>SAFE or UNSAFE</decision>
<reason>
[Your detailed analysis here. If UNSAFE, explain exactly what is problematic and why.]
</reason>
</security_assessment>

<PKGBUILD>
{pkgbuild_content}
</PKGBUILD>"""
    print(f"Auditing PKGBUILD with {MODEL}...")

    logger.info("--- LLM REQUEST ---")
    logger.debug(prompt)

    try:
        assessment = session.get_llm_assessment(prompt)

        logger.info("--- LLM RESPONSE ---")
        logger.debug(assessment)

        root = ET.fromstring(assessment)
        decision = root.find("decision").text.strip().upper()
        reason = root.find("reason").text.strip()

        print("\n--- Security Assessment ---")
        print(f"Decision: {decision}")
        print(f"Reason: {reason}")
        print("---------------------------\n")

        if standalone:
            return decision

        if decision == "UNSAFE":
            print("AUDIT FAILED: Potential security issues detected.", file=sys.stderr)
            sys.exit(1)
        elif decision == "SAFE":
            print("AUDIT PASSED: No obvious security issues detected.")
            print("Proceeding with makepkg execution...")
            os.execv("/usr/bin/makepkg", makepkg_args)
        else:
            print(
                f"ERROR: Unknown decision '{decision}' in LLM response.",
                file=sys.stderr,
            )
            sys.exit(1)

    except (APIError, ET.ParseError, AttributeError) as e:
        print(f"ERROR: LLM response processing failed: {e}", file=sys.stderr)
        if standalone:
            return "UNSAFE"
        sys.exit(1)
    return "UNSAFE"  # Should not be reached


def do_pkgbuild_audit(tmpdir, session):
    print("\n--- Auditing PKGBUILD ---")
    pkgbuild_path = Path(tmpdir) / "PKGBUILD"
    session.add_bytes(pkgbuild_path.stat().st_size)
    return audit_pkgbuild(pkgbuild_path, session, standalone=True) == "SAFE"


def do_changelog_audit(tmpdir, session):
    print("\n--- Auditing Git Commit History ---")
    log_process = subprocess.Popen(
        ["git", "log", "-p", "--pretty=fuller"],
        stdout=subprocess.PIPE,
        text=True,
        cwd=tmpdir,
    )
    current_commit_diff = ""
    for line in iter(log_process.stdout.readline, ""):
        if session.limit_reached:
            break
        session.add_bytes(len(line.encode("utf-8")))
        if line.startswith("commit ") and current_commit_diff:
            if not audit_git_commit(current_commit_diff, session):
                return False
            current_commit_diff = ""
        current_commit_diff += line
    if current_commit_diff and not session.limit_reached:
        if not audit_git_commit(current_commit_diff, session):
            return False
    log_process.stdout.close()
    log_process.wait()
    print("--- Git Commit History Audit Complete ---")
    return True


def audit_git_commit(commit_diff, session):
    """Audits a git commit diff with an LLM."""
    print(
        f"[AUDIT] Checking git commit diff (size: {len(commit_diff.encode('utf-8'))} bytes)..."
    )

    prompt = f"""You are a security expert analyzing a git commit diff from an AUR package.
Please identify any potential security issues, such as:
- Suspicious changes to download URLs or checksums.
- Introduction of malicious or obfuscated code.
- Addition of backdoors or vulnerabilities.
- Any changes that seem out of place for the package's purpose.

Respond with a security assessment in the following XML format:
<security_assessment>
<decision>SAFE or UNSAFE</decision>
<reason>
[Your detailed analysis here. If UNSAFE, explain exactly what is problematic and why.]
</reason>
</security_assessment>

<git_diff>
{commit_diff}
</git_diff>"""

    try:
        assessment = session.get_llm_assessment(prompt)
        root = ET.fromstring(assessment)
        decision = root.find("decision").text.strip().upper()
        if decision == "UNSAFE":
            reason = root.find("reason").text.strip()
            print(f"AUDIT FAILED for git commit: {reason}", file=sys.stderr)
            return False
        return True
    except (APIError, ET.ParseError, AttributeError) as e:
        print(f"ERROR: Could not audit git commit: {e}", file=sys.stderr)
        return False


def do_agentic_audit(tmpdir, session):
    """Performs an agentic security audit on the package contents."""
    print("\n--- Performing Agentic Security Audit ---")
    workdir = Path(tmpdir).resolve()

    try:
        # Use ls -R to get a recursive listing.
        initial_listing = subprocess.check_output(
            ["ls", "-R"],
            cwd=workdir,
            text=True,
            stderr=subprocess.STDOUT,
        )
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Failed to list directory {workdir}: {e.output}", file=sys.stderr)
        return False

    system_prompt = f"""You are an agentic security auditor. Your goal is to inspect the files in the current directory to find any potential vulnerabilities, malicious code, or supply chain attack vectors.

    You have access to the following tools:
    <tools>
      <tool>
        <name>listdir</name>
        <description>List files and directories.</description>
        <parameters>
          <parameter>
            <name>path</name>
            <type>string</type>
            <description>The path to the directory to list. Use '.' for the current directory. This is equivalent to 'ls -F'.</description>
          </parameter>
        </parameters>
      </tool>
      <tool>
        <name>readfile</name>
        <description>Read the content of a file.</description>
        <parameters>
          <parameter>
            <name>path</name>
            <type>string</type>
            <description>The path to the file to read.</description>
          </parameter>
        </parameters>
      </tool>
    </tools>

    To use a tool, you MUST respond with an XML block like this, and nothing else:
    <tool_call>
      <name>tool_name</name>
      <parameters>
        <path>path/to/file_or_dir</path>
      </parameters>
    </tool_call>

    After you have finished your investigation, you MUST respond with a final security assessment in the following format:
    <security_assessment>
      <decision>SAFE or UNSAFE</decision>
      <reason>
      [Your detailed analysis here. If UNSAFE, explain exactly what is problematic and why, including the path to the problematic file(s).]
      </reason>
    </security_assessment>

    The investigation starts now. The current working directory is the root of the cloned AUR package. Here is the initial recursive directory listing:
    <tool_result>
      <name>initial_listing</name>
      <output>
{initial_listing}
      </output>
    </tool_result>
    """

    messages = [{"role": "system", "content": system_prompt}]
    max_tool_calls = 15
    tool_calls = 0

    while tool_calls < max_tool_calls:
        tool_calls += 1
        if session.limit_reached:
            print("WARN: Audit session limit reached, stopping agent.", file=sys.stderr)
            # Ask for a final decision based on info gathered so far.
            messages.append(
                {
                    "role": "user",
                    "content": "Session byte limit reached. Please make a final decision based on the information you have gathered so far.",
                }
            )

        try:
            response = session.audit_prompt(
                model=MODEL,
                messages=messages,
                temperature=0.0,
                top_p=0.1,
            )
            response_content = response.choices[0].message.content
            messages.append({"role": "assistant", "content": response_content})

            if "<security_assessment>" in response_content:
                print("Agent has concluded the audit.")
                # Clean up the response content to ensure it's valid XML
                assessment_text = response_content[
                    response_content.find("<security_assessment>") :
                ]
                root = ET.fromstring(assessment_text)
                decision = root.find("decision").text.strip().upper()
                reason = root.find("reason").text.strip()
                print("\n--- Agent Security Assessment ---")
                print(f"Decision: {decision}")
                print(f"Reason: {reason}")
                print("---------------------------------\n")
                return decision == "SAFE"

            if (
                session.limit_reached
            ):  # If limit was reached, we forced a decision. Don't process tool calls.
                print(
                    "ERROR: Agent did not provide a final assessment after session limit was reached.",
                    file=sys.stderr,
                )
                return False

            tool_call_root = ET.fromstring(response_content)
            if tool_call_root.tag != "tool_call":
                print(
                    f"ERROR: Unexpected agent response: {response_content}",
                    file=sys.stderr,
                )
                return False

            tool_name_elem = tool_call_root.find("name")
            tool_path_elem = tool_call_root.find("parameters/path")

            if tool_name_elem is None or tool_path_elem is None:
                print(
                    f"ERROR: Malformed tool call: {response_content}", file=sys.stderr
                )
                return False

            tool_name = tool_name_elem.text.strip()
            tool_path_param = tool_path_elem.text.strip()

            # Prevent directory traversal.
            # The agent should work inside the temp dir.
            requested_path = (workdir / tool_path_param).resolve()
            if workdir not in requested_path.parents and requested_path != workdir:
                tool_output = f"ERROR: Attempted directory traversal: {tool_path_param}"
                print(tool_output, file=sys.stderr)
            else:
                tool_output = ""
                if tool_name == "listdir":
                    print(f"Agent action: listdir {tool_path_param}")
                    try:
                        tool_output = subprocess.check_output(
                            ["ls", "-F", str(requested_path)],
                            text=True,
                            stderr=subprocess.STDOUT,
                        )
                    except subprocess.CalledProcessError as e:
                        tool_output = f"ERROR: {e.output}"
                elif tool_name == "readfile":
                    print(f"Agent action: readfile {tool_path_param}")
                    try:
                        if not requested_path.is_file():
                            raise FileNotFoundError(f"Not a file: {tool_path_param}")
                        file_size = requested_path.stat().st_size
                        session.add_bytes(file_size)
                        if session.limit_reached:
                            tool_output = (
                                "ERROR: Session audit limit reached. Cannot read file."
                            )
                        else:
                            with open(requested_path, "r", errors="ignore") as f:
                                tool_output = f.read()
                    except Exception as e:
                        tool_output = f"ERROR: {e}"
                else:
                    tool_output = f"ERROR: Unknown tool '{tool_name}'"

            tool_result_message = f"""<tool_result>
<name>{tool_name}</name>
<output>
{tool_output}
</output>
</tool_result>"""
            messages.append({"role": "user", "content": tool_result_message})

        except (APIError, ET.ParseError, AttributeError) as e:
            print(f"ERROR: Agentic audit failed: {e}", file=sys.stderr)
            # For debugging, show the last message from the assistant
            if messages and messages[-1]["role"] == "assistant":
                print("Last assistant message:", file=sys.stderr)
                print(messages[-1]["content"], file=sys.stderr)
            return False
        except Exception as e:
            print(
                f"ERROR: An unexpected error occurred in the agent loop: {e}",
                file=sys.stderr,
            )
            return False

    print(f"ERROR: Agent exceeded max tool calls ({max_tool_calls}).", file=sys.stderr)
    return False


def run_aur_sleuth_audit(package_name, audit_levels, client):
    """Runs the specified security audits for aur-sleuth."""
    tmpdir = tempfile.mkdtemp(prefix="aur-sleuth-")
    print(f"Created temporary directory: {tmpdir}")
    status = "error"
    original_cwd = os.getcwd()
    session = AuditSession(limit_bytes=SESSION_AUDIT_LIMIT_BYTES, client=client)
    try:
        print(f"Cloning https://aur.archlinux.org/{package_name}.git...")
        subprocess.run(
            ["git", "clone", f"https://aur.archlinux.org/{package_name}.git", tmpdir],
            check=True,
            capture_output=True,
            text=True,
        )
        os.chdir(tmpdir)

        audit_ok = True
        if "agentic" in audit_levels:
            print("\nRunning makepkg --nobuild to download sources for agent...")
            try:
                subprocess.run(
                    ["makepkg", "--nobuild"], check=True, capture_output=True, text=True
                )
            except subprocess.CalledProcessError as e:
                print(
                    "makepkg --nobuild failed, continuing with only PKGBUILD.",
                    file=sys.stderr,
                )
                print(e.stderr, file=sys.stderr)

            audit_ok = do_agentic_audit(tmpdir, session)
        else:
            if "PKGBUILD" in audit_levels or "hardcore" in audit_levels:
                audit_ok = do_pkgbuild_audit(tmpdir, session)
            if audit_ok and ("changelog" in audit_levels or "hardcore" in audit_levels):
                audit_ok = do_changelog_audit(tmpdir, session)
        else:
            print("Invalid audit type")
            audit_ok = False

        if audit_ok:
            print("\nAudit passed. You may inspect the files and run makepkg manually.")
            print(f"Temporary directory: {tmpdir}")
            status = "success"
        else:
            print("\nAUDIT FAILED. See reasons above.", file=sys.stderr)

    except (subprocess.CalledProcessError, Exception) as e:
        print(f"ERROR: An unexpected error occurred: {e}", file=sys.stderr)
    finally:
        session.print_token_usage()
        os.chdir(original_cwd)
        if status == "error" and os.path.exists(tmpdir):
            print(f"Cleaning up temporary directory: {tmpdir}")
            shutil.rmtree(tmpdir)


if __name__ == "__main__":
    invocation_name = Path(sys.argv[0]).name
    base_url = get_base_url()
    default_headers = {}
    if "openrouter.ai" in base_url:
        default_headers = {
            "HTTP-Referer": "https://github.com/mgalgs/aur-sleuth",
            "X-Title": "aur-sleuth",
        }

    client = OpenAI(
        api_key=get_api_key(), base_url=base_url, default_headers=default_headers
    )

    if invocation_name == "aur-sleuth":
        parser = argparse.ArgumentParser(
            description="Run a security audit on an AUR package."
        )
        parser.add_argument("package_name", help="Name of the AUR package.")
        parser.add_argument(
            "--audit",
            nargs="+",
            default=["PKGBUILD"],
            choices=["PKGBUILD", "changelog", "hardcore", "agentic"],
            help="Specify audit level(s). 'agentic' runs a special agent-based audit.",
        )
        args = parser.parse_args()
        run_aur_sleuth_audit(args.package_name, args.audit, client)
    elif invocation_name == "aur-sleuth-makepkg-wrapper":
        parser = argparse.ArgumentParser(
            description="Wrapper for makepkg to audit PKGBUILD."
        )
        parser.add_argument(
            "-p",
            "--pkgbuild",
            dest="pkgbuild_path",
            default="PKGBUILD",
            help="Path to PKGBUILD.",
        )
        args, makepkg_args = parser.parse_known_args()

        # Arguments that indicate a non-build action where we should skip the audit
        skip_audit_flags = ["--verifysource", "--nobuild", "--geninteg", "-o", "-g"]

        # Check if any of the skip flags are in the arguments passed to the wrapper
        if any(flag in makepkg_args for flag in skip_audit_flags):
            # If so, just execute makepkg without an audit
            os.execv("/usr/bin/makepkg", ["/usr/bin/makepkg"] + makepkg_args)
        else:
            # Otherwise, proceed with the audit
            session = AuditSession(limit_bytes=SESSION_AUDIT_LIMIT_BYTES, client=client)
            pkgbuild_path = Path(args.pkgbuild_path).resolve()
            # Re-add the main makepkg command to the arguments list
            makepkg_args.insert(0, "/usr/bin/makepkg")
            audit_pkgbuild(pkgbuild_path, session, makepkg_args)
            session.print_token_usage()
    else:
        print(
            f"ERROR: Unknown invocation name '{invocation_name}'. Please invoke as 'aur-sleuth' or 'aur-sleuth-makepkg-wrapper'.",
            file=sys.stderr,
        )
        sys.exit(1)
